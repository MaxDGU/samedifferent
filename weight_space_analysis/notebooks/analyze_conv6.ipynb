{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41701601",
   "metadata": {},
   "source": [
    "# Conv6 Weight Space Analysis\n",
    "\n",
    "This notebook analyzes and visualizes the weight spaces of Conv6 models, comparing vanilla and meta-learned variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5b3f7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import our dependencies and setup our analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f81df1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'umap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../src\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install scikit-learn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweight_analyzer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeightSpaceAnalyzer, WeightSpaceVisualizer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[1;32m      6\u001b[0m pio\u001b[38;5;241m.\u001b[39mtemplates\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplotly_white\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Princeton/CoCoSci_Lab/samedifferent/same_different_paper/metasamedifferent/weight_space_analysis/notebooks/../src/weight_analyzer.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Tuple, Optional, Union\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'umap'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "!pip install umap\n",
    "from weight_analyzer import WeightSpaceAnalyzer, WeightSpaceVisualizer\n",
    "import plotly.io as pio\n",
    "pio.templates.default = 'plotly_white'\n",
    "\n",
    "# For better notebook display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c5043d",
   "metadata": {},
   "source": [
    "## Load Model Weights\n",
    "First, we'll load the weights for both vanilla and meta-learned Conv6 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d34122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzers\n",
    "vanilla_analyzer = WeightSpaceAnalyzer('vanilla', 'conv6')\n",
    "meta_analyzer = WeightSpaceAnalyzer('meta', 'conv6')\n",
    "\n",
    "# Load weights for all seeds\n",
    "seeds = list(range(42, 47))  # Seeds 42-46\n",
    "base_path = '../..'\n",
    "\n",
    "print(\"Loading vanilla weights...\")\n",
    "vanilla_analyzer.load_weights(base_path, seeds)\n",
    "print(\"\\nLoading meta-learned weights...\")\n",
    "meta_analyzer.load_weights(base_path, seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f8383f",
   "metadata": {},
   "source": [
    "## Weight Distribution Analysis\n",
    "Let's analyze the distribution of weights in each convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = WeightSpaceVisualizer()\n",
    "\n",
    "# Analyze each conv layer\n",
    "conv_layers = [f'conv{i}.weight' for i in range(1, 7)]\n",
    "\n",
    "for layer in conv_layers:\n",
    "    print(f\"\\nAnalyzing {layer}...\")\n",
    "    \n",
    "    # Extract weights\n",
    "    vanilla_weights = vanilla_analyzer.extract_layer_weights(layer)\n",
    "    meta_weights = meta_analyzer.extract_layer_weights(layer)\n",
    "    \n",
    "    # Create distribution plot\n",
    "    dist_fig = visualizer.create_distribution_plot(vanilla_weights, meta_weights, layer)\n",
    "    dist_fig.show()\n",
    "    \n",
    "    # Compute and display statistics\n",
    "    print(f\"\\nLayer: {layer}\")\n",
    "    print(\"\\nVanilla Statistics:\")\n",
    "    for seed, weights in vanilla_weights.items():\n",
    "        stats = vanilla_analyzer.compute_statistics(weights)\n",
    "        print(f\"Seed {seed}: {stats}\")\n",
    "    \n",
    "    print(\"\\nMeta Statistics:\")\n",
    "    for seed, weights in meta_weights.items():\n",
    "        stats = meta_analyzer.compute_statistics(weights)\n",
    "        print(f\"Seed {seed}: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b759f34",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Analysis\n",
    "Now we'll visualize the weight spaces using different dimensionality reduction techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2414de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_methods = ['pca', 'tsne', 'umap']\n",
    "\n",
    "for layer in conv_layers:\n",
    "    print(f\"\\nAnalyzing {layer} with dimensionality reduction...\")\n",
    "    \n",
    "    # Extract weights\n",
    "    vanilla_weights = vanilla_analyzer.extract_layer_weights(layer)\n",
    "    meta_weights = meta_analyzer.extract_layer_weights(layer)\n",
    "    \n",
    "    for method in reduction_methods:\n",
    "        print(f\"\\nApplying {method.upper()}...\")\n",
    "        \n",
    "        # Reduce dimensions\n",
    "        vanilla_embeddings = {}\n",
    "        meta_embeddings = {}\n",
    "        \n",
    "        for seed, weights in vanilla_weights.items():\n",
    "            vanilla_embeddings[seed] = vanilla_analyzer.reduce_dimensions(weights, method=method)\n",
    "        \n",
    "        for seed, weights in meta_weights.items():\n",
    "            meta_embeddings[seed] = meta_analyzer.reduce_dimensions(weights, method=method)\n",
    "        \n",
    "        # Create embedding plot\n",
    "        emb_fig = visualizer.create_embedding_plot(\n",
    "            vanilla_embeddings, meta_embeddings, layer, method\n",
    "        )\n",
    "        emb_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565948bf",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "We can save any interesting figures or statistics for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code here to save specific visualizations or compile statistics "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
